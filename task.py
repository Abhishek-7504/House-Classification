# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zKf2bmRrAcmenpts32KDKtQgV7Iby80y
"""

# Importing the necessary libraries
import pandas as pd
import re
import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

# Importing the dataset, as it is a Excel File, I have used "pd.read_excel"
train_df = pd.read_excel('task_dataset.xlsx', sheet_name='training_dataset')
validation_df = pd.read_excel('task_dataset.xlsx', sheet_name='validation_dataset')

# Understanding the dataset
print("Training dataset: ", train_df.info())
print("\n")
print("Validation dataset: ", validation_df.info())

# Cleaning the data
def clean_address(text):
  if not isinstance(text, str):
    return ""
  text = text.lower()
  text = re.sub(r'[^a-z0-9\s]', ' ', text)
  text = re.sub(r'\s+', ' ', text).strip()
  return text

train_df['clean_text'] = train_df['property_address'].apply(clean_address)
validation_df['clean_text'] = validation_df['property_address'].apply(clean_address)

# Using Logistic Regression
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')),
    ('clf', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))
])

print("Training Model...")
pipeline.fit(train_df['clean_text'], train_df['categories'])
print()
print("Evaluating... ")
y_pred = pipeline.predict(validation_df['clean_text'])

print("Classisfication Report: ")
print(classification_report(validation_df['categories'], y_pred))

print("Confusion Matrix: ")
print(confusion_matrix(validation_df['categories'], y_pred))

joblib.dump(pipeline, 'address_classifier.pkl')
print("Model saved as 'Address Classifier'")

# Using BERT to try and increase the accuracy
# Preparing data for using BERT
import numpy as np # Import numpy
labels = sorted(train_df['categories'].unique())
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for i, label in enumerate(labels)}

train_df['labels'] = train_df['categories'].map(label2id)
validation_df['labels'] = validation_df['categories'].map(label2id)

train_dataset = Dataset.from_pandas(train_df[['clean_text', 'labels']])
validation_dataset = Dataset.from_pandas(validation_df[['clean_text', 'labels']])

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize_function(examples):
  # Padding and Truncation are crucial for BERT
  return tokenizer(examples['clean_text'], padding="max_length", truncation=True, max_length=64)

print("Tokenizing data...")
tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val = validation_dataset.map(tokenize_function, batched=True)

model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id
)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    learning_rate=2e-5,
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {"accuracy": accuracy_score(labels, predictions)}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    compute_metrics=compute_metrics,
)

print("Starting Training...")
trainer.train()

print("\nEvaluating...")
# Get raw predictions
predictions_output = trainer.predict(tokenized_val)
y_pred_indices = np.argmax(predictions_output.predictions, axis=1)
y_true_indices = predictions_output.label_ids

# Convert indices back to text labels for the report
y_pred_labels = [id2label[i] for i in y_pred_indices]
y_true_labels = [id2label[i] for i in y_true_indices]

print("Classification Report: ")
print(classification_report(y_true_labels, y_pred_labels))

print("Confusion Matrix: ")
print(confusion_matrix(y_true_labels, y_pred_labels))

model_path = "./address_classifier_bert"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)
print(f"Model saved to '{model_path}'")

